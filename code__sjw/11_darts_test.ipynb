{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from fbprophet import Prophet\n",
    "import pickle\n",
    "import copy\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from darts.models import *\n",
    "from darts.models.forecasting import gradient_boosted_model\n",
    "import darts.utils.timeseries_generation as tg\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "from darts.datasets import EnergyDataset\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.timeseries import TimeSeries\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 769 ms, sys: 140 ms, total: 910 ms\n",
      "Wall time: 910 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submit = json.load(open('./data/sample_submission/sample_submission.json', 'r', encoding='utf8')) \n",
    "df_dic = pickle.load(open('./mydata/df_dic.plk','rb'))\n",
    "plc_lst = pickle.load(open('./mydata/plc_lst.plk','rb'))\n",
    "fct_lst = ['pH', 'COD', 'SS', 'N', 'P', 'T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.32 s, sys: 7.66 ms, total: 2.33 s\n",
      "Wall time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "''' fillna : 단순 missing value 채움'''\n",
    "for plc in plc_lst:\n",
    "    df = df_dic[plc]\n",
    "    df = df.fillna(method='ffill')\n",
    "    df = df.fillna(method='bfill')    \n",
    "    df_dic[plc] = df.dropna()\n",
    "    \n",
    "    if df_dic[plc].isna().any().any():\n",
    "        print(plc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prophet 예측을 위한 dataframe 포맷 제작\n",
    "future = pd.DataFrame([str(x)[:10] for x in list(pd.date_range(start='2018-02-01', end='2019-12-31', inclusive=\"both\"))], columns=['ds'])\n",
    "future['y'] = np.nan\n",
    "#future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 예측 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfall_comm= pickle.load( open('./mydata/dfall_comm.plk','rb'))\n",
    "df_grp = dfall_comm.groupby('loc')\n",
    "df_temp = df_grp.get_group(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models.forecasting import tcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 : kalman\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(plc_lst):\n",
    "    plc_df = df_dic[i]\n",
    "    \n",
    "    for j in fct_lst: # 피쳐 리스트\n",
    "        fct_df = plc_df[['ds', j]]   # 날짜 - 피쳐1 조합임.\n",
    "        \n",
    "        fct_df = fct_df.rename(columns={j:'y'}) # 피쳐1을 y로 바꾼다.\n",
    "        fct_df['ds'] = fct_df.ds.apply(lambda x : datetime.strptime(x , '%Y%m%d'))\n",
    "        fct_df = fct_df.dropna()\n",
    "        fct_df['y'] = pd.to_numeric(fct_df.y)\n",
    "        \n",
    "        df_align = pd.DataFrame([ x for x in list(pd.date_range( end = '2018-01-31', periods = len(fct_df), inclusive=\"both\"))], columns=['ds'])\n",
    "        df_align['y'] = fct_df.y.values\n",
    "        \n",
    "        # TimeSeris 로 변환 하기 \n",
    "        dfts = TimeSeries.from_dataframe(df_align,  time_col = 'ds' , value_cols  = ['y'], fill_missing_dates=True  , freq='D')\n",
    "        \n",
    "        diff = datetime(2018,2,1) - df_align.loc[0].ds\n",
    "        if diff.days > 720:\n",
    "            lag = 700\n",
    "        else:\n",
    "            lag = diff.days - 10\n",
    "            \n",
    "        my_model = tcn_model.TCNModel(\n",
    "            input_chunk_length = 500,\n",
    "            output_chunk_length = 1,\n",
    "            n_epochs=50,\n",
    "            kernel_size =3,\n",
    "            num_filters = 12,\n",
    "            dropout = 0.3,\n",
    "            batch_size = 64,\n",
    "            pl_trainer_kwargs  = {\"accelerator\": \"gpu\", \"gpus\": -1, \"auto_select_gpus\": True} ,\n",
    "            dilation_base=2,\n",
    "            weight_norm=True,\n",
    "            random_state=2022,\n",
    "            \n",
    "            \n",
    "        )\n",
    "        \n",
    "        my_model.fit(dfts)\n",
    "        pred_series = my_model.predict( n = 699 )\n",
    "        \n",
    "        forecast = pred_series.pd_dataframe()\n",
    "        forecast['ds'] = forecast.index\n",
    "        forecast['ds'] = forecast['ds'].apply(lambda x : str(x).split(' ')[0].split('-')[0]+str(x).split(' ')[0].split('-')[1]+str(x).split(' ')[0].split('-')[2])\n",
    "        forecast.columns = ['yhat', 'ds']\n",
    "        forecast = forecast.reset_index(drop=True)\n",
    "        \n",
    "        for idx,k in enumerate(submit[i].keys()):\n",
    "            submit[i][k][j] = np.round(forecast.yhat[idx],6)\n",
    "      \n",
    "        \n",
    "end_time = time.time()\n",
    "print('소요시간 :', (end_time - start_time)/60, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models.forecasting import nhits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a1398bd9854fdbb39c00b8ac0e6a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-14 21:56:13,939] WARNING | darts.models.forecasting.torch_forecasting_model | DeprecationWarning: `torch_device_str` is deprecated and will be removed in a coming Darts version. For full support of all torch devices, use PyTorch-Lightnings trainer flags and pass them inside `pl_trainer_kwargs`. Flags of interest are {`accelerator`, `gpus`, `auto_select_gpus`, `devices`}. For more information, visit https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags\n",
      "[2022-06-14 21:56:13,939] WARNING | darts.models.forecasting.torch_forecasting_model | DeprecationWarning: `torch_device_str` is deprecated and will be removed in a coming Darts version. For full support of all torch devices, use PyTorch-Lightnings trainer flags and pass them inside `pl_trainer_kwargs`. Flags of interest are {`accelerator`, `gpus`, `auto_select_gpus`, `devices`}. For more information, visit https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags\n",
      "2022-06-14 21:56:13 darts.models.forecasting.torch_forecasting_model WARNING: DeprecationWarning: `torch_device_str` is deprecated and will be removed in a coming Darts version. For full support of all torch devices, use PyTorch-Lightnings trainer flags and pass them inside `pl_trainer_kwargs`. Flags of interest are {`accelerator`, `gpus`, `auto_select_gpus`, `devices`}. For more information, visit https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags\n",
      "[2022-06-14 21:56:13,940] INFO | darts.models.forecasting.nhits | (N-HiTS): Using automatic kernel pooling size: ((350,), (18,), (1,)).\n",
      "[2022-06-14 21:56:13,940] INFO | darts.models.forecasting.nhits | (N-HiTS): Using automatic kernel pooling size: ((350,), (18,), (1,)).\n",
      "2022-06-14 21:56:13 darts.models.forecasting.nhits INFO: (N-HiTS): Using automatic kernel pooling size: ((350,), (18,), (1,)).\n",
      "[2022-06-14 21:56:13,940] INFO | darts.models.forecasting.nhits | (N-HiTS):  Using automatic downsampling coefficients: ((1,), (1,), (1,)).\n",
      "[2022-06-14 21:56:13,940] INFO | darts.models.forecasting.nhits | (N-HiTS):  Using automatic downsampling coefficients: ((1,), (1,), (1,)).\n",
      "2022-06-14 21:56:13 darts.models.forecasting.nhits INFO: (N-HiTS):  Using automatic downsampling coefficients: ((1,), (1,), (1,)).\n",
      "[2022-06-14 21:56:13,941] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 2120 samples.\n",
      "[2022-06-14 21:56:13,941] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 2120 samples.\n",
      "2022-06-14 21:56:13 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 2120 samples.\n",
      "[2022-06-14 21:56:13,961] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 64-bits; casting model to float64.\n",
      "[2022-06-14 21:56:13,961] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 64-bits; casting model to float64.\n",
      "2022-06-14 21:56:13 darts.models.forecasting.torch_forecasting_model INFO: Time series values are 64-bits; casting model to float64.\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "GPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18209/1754595060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mpred_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m699\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/darts/utils/torch.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfork_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_TORCH_SEED_VALUE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/darts/models/forecasting/torch_forecasting_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, num_loader_workers)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train dataset contains {len(train_dataset)} samples.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         return self.fit_from_dataset(\n\u001b[0m\u001b[1;32m    772\u001b[0m             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_loader_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         )\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/darts/utils/torch.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfork_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_TORCH_SEED_VALUE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/darts/models/forecasting/torch_forecasting_model.py\u001b[0m in \u001b[0;36mfit_from_dataset\u001b[0;34m(self, train_dataset, val_dataset, trainer, verbose, epochs, num_loader_workers)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# setup trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;31m# TODO: multiple training without loading from checkpoint is not trivial (I believe PyTorch-Lightning is still\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/darts/models/forecasting/torch_forecasting_model.py\u001b[0m in \u001b[0;36m_setup_trainer\u001b[0;34m(self, trainer, verbose, epochs)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         self.trainer = (\n\u001b[0;32m--> 524\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/darts/models/forecasting/torch_forecasting_model.py\u001b[0m in \u001b[0;36m_init_trainer\u001b[0;34m(trainer_params, max_epochs)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mtrainer_params_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrainer_params_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/argparse.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple_trainloader_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         self._accelerator_connector = AcceleratorConnector(\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_flag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"auto\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_flag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_choose_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_parallel_devices_and_init_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# 3. Instantiate ClusterEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd/jaewoo/anaconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m_set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    512\u001b[0m                 \u001b[0macc_str\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0macc_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_types\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mAcceleratorRegistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             ]\n\u001b[0;32m--> 514\u001b[0;31m             raise MisconfigurationException(\n\u001b[0m\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"{self.accelerator.__class__.__qualname__} can not run on your system\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;34m\" since the accelerator is not available. The following accelerator(s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: GPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
     ]
    }
   ],
   "source": [
    "# 추론 : rf\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(plc_lst):\n",
    "    plc_df = df_dic[i]\n",
    "    \n",
    "    for j in fct_lst: # 피쳐 리스트\n",
    "        fct_df = plc_df[['ds', j]]   # 날짜 - 피쳐1 조합임.\n",
    "        \n",
    "        fct_df = fct_df.rename(columns={j:'y'}) # 피쳐1을 y로 바꾼다.\n",
    "        fct_df['ds'] = fct_df.ds.apply(lambda x : datetime.strptime(x , '%Y%m%d'))\n",
    "        fct_df = fct_df.dropna()\n",
    "        fct_df['y'] = pd.to_numeric(fct_df.y)\n",
    "        \n",
    "        df_align = pd.DataFrame([ x for x in list(pd.date_range( end = '2018-01-31', periods = len(fct_df), inclusive=\"both\"))], columns=['ds'])\n",
    "        df_align['y'] = fct_df.y.values\n",
    "        \n",
    "        # TimeSeris 로 변환 하기 \n",
    "        dfts = TimeSeries.from_dataframe(df_align,  time_col = 'ds' , value_cols  = ['y'], fill_missing_dates=True  , freq='D')\n",
    "        \n",
    "        diff = datetime(2018,2,1) - df_align.loc[0].ds\n",
    "        if diff.days > 720:\n",
    "            lag = 700\n",
    "        else:\n",
    "            lag = diff.days - 10\n",
    "            \n",
    "        my_model = nhits.NHiTS(\n",
    "            input_chunk_length = lag,\n",
    "            output_chunk_length = 1,\n",
    "\n",
    "            n_epochs=20,\n",
    "            nr_epochs_val_period=1,\n",
    "            batch_size=256,\n",
    "            torch_device_str=\"cuda\",\n",
    "            random_state=2022\n",
    "        )\n",
    "        \n",
    "        my_model.fit(dfts)\n",
    "        pred_series = my_model.predict( n = 699 )\n",
    "        \n",
    "        forecast = pred_series.pd_dataframe()\n",
    "        forecast['ds'] = forecast.index\n",
    "        forecast['ds'] = forecast['ds'].apply(lambda x : str(x).split(' ')[0].split('-')[0]+str(x).split(' ')[0].split('-')[1]+str(x).split(' ')[0].split('-')[2])\n",
    "        forecast.columns = ['yhat', 'ds']\n",
    "        forecast = forecast.reset_index(drop=True)\n",
    "        \n",
    "        for idx,k in enumerate(submit[i].keys()):\n",
    "            submit[i][k][j] = np.round(forecast.yhat[idx],6)\n",
    "      \n",
    "        \n",
    "end_time = time.time()\n",
    "print('소요시간 :', (end_time - start_time)/60, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 사업장별 dataframe 제작\n",
    "df_dic = {}\n",
    "plc_lst = list(train.keys())\n",
    "fct_lst = ['pH', 'COD', 'SS', 'N', 'P', 'T']\n",
    "\n",
    "for i in plc_lst:\n",
    "    plc_df = pd.DataFrame(columns={'ds', 'pH', 'COD', 'SS', 'N', 'P', 'T'})\n",
    "    plc_df = plc_df[['ds', 'pH', 'COD', 'SS', 'N', 'P', 'T']]\n",
    "    \n",
    "    date_lst = list(train[i].keys())\n",
    "    plc_df.loc[:, 'ds'] = date_lst\n",
    "\n",
    "    for index, j in enumerate(date_lst):\n",
    "        for k in fct_lst:\n",
    "            try:\n",
    "                plc_df.loc[index, k] = train[i][j][k]\n",
    "            except:\n",
    "                pass\n",
    "                    \n",
    "    df_dic[i] = plc_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gb2",
   "language": "python",
   "name": "gb2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
